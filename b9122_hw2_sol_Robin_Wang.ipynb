{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d24ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set-up\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from simplified_scrapy.simplified_doc import SimplifiedDoc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d528fb",
   "metadata": {},
   "source": [
    "### Q1 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb80e9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/en/content/secretary-general/press-release\n",
      "/en/content/general-assembly/press-release\n",
      "/en/content/security-council/press-release\n",
      "/en/content/economic-and-social-council/press-release\n",
      "https://www.icj-cij.org/en/press-releases\n",
      "https://press.un.org/en/content/press-release\n",
      "/en/content/press-release\n"
     ]
    }
   ],
   "source": [
    "### To find the relative press-release page\n",
    "\n",
    "# The original URL\n",
    "base_url = 'https://press.un.org/en'\n",
    "\n",
    "# Fetch the webpage\n",
    "response = requests.get(base_url)\n",
    "\n",
    "# Check if request was successful\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    # Parse the webpage\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all links (internal and external) that contain \"press-release\"\n",
    "    press_release_links = [\n",
    "        a['href'] for a in soup.find_all('a', href=True) \n",
    "        if \"press-release\" in a['href']\n",
    "    ]\n",
    "\n",
    "    # Output the links\n",
    "    for link in press_release_links:\n",
    "        print(link)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce6d42",
   "metadata": {},
   "source": [
    "#### From above results, we notice that the link https://press.un.org/en/content/press-release are the location with all the press-release. Then we do the web scriping from https://press.un.org/en/content/press-release. \n",
    "#### Also when check that website, we should notice that the URL for each page follows a predictable pattern that appending a page number at the end (like https://press.un.org/en/content/press-release?page=1), we will use this pattern to update the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "820b1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### fetch child website from above link\n",
    "\n",
    "def get_press_releases_with_crisis(max_page_number=10):\n",
    "    url_pattern = 'https://press.un.org/en/content/press-release?page={}'\n",
    "    press_releases_with_crisis = []\n",
    "    \n",
    "    for page_number in range(1, max_page_number + 1):\n",
    "        page_url = url_pattern.format(page_number)\n",
    "        response = requests.get(page_url)\n",
    "        \n",
    "        ### fetch child links from each page\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            press_release_candidate_links = soup.find_all('a', href=True)\n",
    "    \n",
    "            for link in press_release_candidate_links:\n",
    "                press_release_url = urljoin('https://press.un.org', link['href'])\n",
    "\n",
    "                try:\n",
    "                    press_release_response = requests.get(press_release_url)\n",
    "                    \n",
    "                except requests.RequestException as e:\n",
    "                    continue\n",
    "                \n",
    "                ### Check the anchor\n",
    "                if press_release_response.status_code == 200:\n",
    "                    press_release_soup = BeautifulSoup(press_release_response.text, 'html.parser')\n",
    "                    specific_anchor = press_release_soup.find('a', {'href': '/en/press-release', 'hreflang': 'en'})\n",
    "                    \n",
    "                    ### Check the word 'crisis'\n",
    "                    if specific_anchor and \"crisis\" in press_release_soup.get_text().lower():\n",
    "                        press_releases_with_crisis.append(press_release_url)\n",
    "                        \n",
    "                        ### Stop when we have 10\n",
    "                        if len(press_releases_with_crisis) >= 10:\n",
    "                            break\n",
    "                            \n",
    "            ### Stop when we have 10\n",
    "            if len(press_releases_with_crisis) >= 10:\n",
    "                break\n",
    "                \n",
    "        else:\n",
    "            print(f\"Failed to retrieve content from {page_url}\")\n",
    "    \n",
    "    return press_releases_with_crisis\n",
    "\n",
    "\n",
    "### function to save the website as txt\n",
    "def save_press_releases(press_releases):\n",
    "    for num, url in enumerate(press_releases, start=1):\n",
    "        response = requests.get(url)\n",
    "        filename = f\"1_{num}.txt\"\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        print(f\"Saved: {url} as 1_{num}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62827b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robin_personal/opt/anaconda3/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: https://press.un.org/en/2023/sgsm21967.doc.htm as 1_1.txt\n",
      "Saved: https://press.un.org/en/2023/dsgsm1877.doc.htm as 1_2.txt\n",
      "Saved: https://press.un.org/en/2023/sgsm21959.doc.htm as 1_3.txt\n",
      "Saved: https://press.un.org/en/2023/sgsm21956.doc.htm as 1_4.txt\n",
      "Saved: https://press.un.org/en/2023/sgsm21952.doc.htm as 1_5.txt\n",
      "Saved: https://press.un.org/en/2023/sgsm21951.doc.htm as 1_6.txt\n",
      "Saved: https://press.un.org/en/2023/sgsm21950.doc.htm as 1_7.txt\n",
      "Saved: https://press.un.org/en/2023/sgsm21947.doc.htm as 1_8.txt\n",
      "Saved: https://press.un.org/en/2023/sgsm21945.doc.htm as 1_9.txt\n",
      "Saved: https://press.un.org/en/2023/dsgsm1874.doc.htm as 1_10.txt\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "press_releases_with_crisis = get_press_releases_with_crisis(10)\n",
    "save_press_releases(press_releases_with_crisis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3e7912",
   "metadata": {},
   "source": [
    "### Q1 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3b85bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### fetch all links from the page\n",
    "def crawl_page(base_url, page_number):\n",
    "    page_url = f\"{base_url}{page_number}\"\n",
    "    source_code = requests.get(page_url)\n",
    "    doc = SimplifiedDoc(source_code.content.decode('utf-8'))\n",
    "    lst = doc.listA(url=page_url)\n",
    "    return [a['url'] for a in lst]\n",
    "\n",
    "### check the archor\n",
    "def is_press_release(url):\n",
    "    source_code = requests.get(url)\n",
    "    return '<span class=\"ep_name\">Plenary session</span>' in source_code.text\n",
    "\n",
    "### check the word \n",
    "def contains_word(url, word):\n",
    "    source_code = requests.get(url)\n",
    "    return word.lower() in source_code.text.lower()\n",
    "\n",
    "### go through all links \n",
    "def find_press_releases_with_word(base_url, word, max_results=10):\n",
    "    press_releases_with_word = []\n",
    "    page_number = 0\n",
    "    \n",
    "    while len(press_releases_with_word) < max_results:\n",
    "\n",
    "        urls = crawl_page(base_url, page_number)\n",
    "        press_release_urls = [url for url in urls if is_press_release(url)]\n",
    "        \n",
    "        for url in press_release_urls:\n",
    "            if len(press_releases_with_word) >= max_results:\n",
    "                break\n",
    "            if contains_word(url, word):\n",
    "                press_releases_with_word.append(url)\n",
    "                print(f\"Found '{word}' in press release: {url}\")\n",
    "        \n",
    "        page_number += 1\n",
    "        \n",
    "    return press_releases_with_word[:max_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e74ed6aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'crisis' in press release: https://www.europarl.europa.eu/news/en/press-room/20230929ipr06132/nagorno-karabakh-meps-demand-review-of-eu-relations-with-azerbaijan\n",
      "Found 'crisis' in press release: https://www.europarl.europa.eu/news/en/press-room/20230929ipr06130/parliament-argues-for-a-top-up-to-multi-annual-budget-for-crisis-response\n",
      "Found 'crisis' in press release: https://www.europarl.europa.eu/news/en/press-room/20230911ipr04923/reduce-demand-and-protect-people-in-prostitution-say-meps\n",
      "Found 'crisis' in press release: https://www.europarl.europa.eu/news/en/press-room/20230911ipr04918/svietlana-tsikhanouskaya-to-meps-support-belarusians-european-aspirations\n",
      "Found 'crisis' in press release: https://www.europarl.europa.eu/news/en/press-room/20230911ipr04908/meps-vote-to-strengthen-eu-defence-industry-through-common-procurement\n",
      "Found 'crisis' in press release: https://www.europarl.europa.eu/news/en/press-room/20230707ipr02427/covid-19-parliament-adopts-roadmap-to-better-prepare-for-future-health-crises\n",
      "Found 'crisis' in press release: https://www.europarl.europa.eu/news/en/press-room/20230707ipr02421/parliament-adopts-new-rules-to-boost-energy-savings\n",
      "Found 'crisis' in press release: https://www.europarl.europa.eu/news/en/press-room/20230707ipr02418/semiconductors-meps-adopt-legislation-to-boost-eu-chips-industry\n",
      "Found 'crisis' in press release: https://www.europarl.europa.eu/news/en/press-room/20230706ipr02317/ep-today\n",
      "Found 'crisis' in press release: https://www.europarl.europa.eu/news/en/press-room/20230706ipr02316/ep-today\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "base_url = 'https://www.europarl.europa.eu/news/en/press-room/page/'\n",
    "word = 'crisis'\n",
    "press_releases = find_press_releases_with_word(base_url, word, 10)\n",
    "\n",
    "num = 1\n",
    "for url in press_releases:\n",
    "\n",
    "    response = requests.get(url)\n",
    "    filename = f\"2_{num}.txt\"\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(response.text)\n",
    "    num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
